{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ4Xt1uf1LT7"
   },
   "source": [
    "# Final Project Phase 1 Summary\n",
    "This Jupyter Notebook (.ipynb) will serve as the skeleton file for your submission for Phase 1 of the Final Project. Answer all statements addressed below as specified in the instructions for the project, covering all necessary details. Please be clear and concise in your answers. Each response should be at most 3 sentences. Good luck! <br><br>\n",
    "\n",
    "Note: To edit a Markdown cell, double-click on its text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8OjIe2z1LT8"
   },
   "source": [
    "## About Your Topic\n",
    "<br>\n",
    "<b>Q1) What topic have you chosen for your final project?</b> <br>\n",
    "For this project, I have chosen to analyze economic data as it relates to the COVID-19 pandemic. Specifically, I will be comparing Covid case rates with unemployment rates for each county in the United States. <br> <br>\n",
    "<b>Q2) Why did you choose this specific topic and what are you looking to learn from the analysis? </b> <br>\n",
    "One of my last competitions in high school was for the NY Fed Challenge, which is essentially an economic analysis - this happened on the 2nd week of March, so we were just starting to see (and had a lot to predict) about the impacts of the pandemic. I consider this project to be a follow-up to my earlier look at the data, and hope to see how significantly the pandemic affected unemployment rates. The hope is to be able to determine whether a correlation exists between case rates and unemployment rates, and if so, quantify the strength of the correlation.<br> <br>\n",
    "<b>Q3) Explain some of the concrete insights you expect to gather from your data and/or hypothesis you expect to answer. </b> <br>\n",
    "I hope to derive county-level connections between changes in the unemployment rate and new Covid cases. Data that I sourced includes monthly unemployment rates per county, daily cumulative Covid cases by county, and county populations. I would like to examine whether monthly case rates (adjusted by population per county) have any relation to monthly unemployment rates by county, and if so, determine the strength of the relationship between these two variables.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJkKO79R1LT9"
   },
   "source": [
    "## About Your Data\n",
    "<br>\n",
    "<b>Q4) Which dataset(s) and websites have you chosen to utilize for your analysis? Include the relevant filenames and links to your datasets. (Please follow the .ipynb hyperlink formatting for links, shown below.) </b> <br>\n",
    "For my static file, I found a csv file that listed cumulative confirmed Covid case counts for each county in the United States, for every day since late January. This file, titled \"covid_confirmed_usafacts\", can be downloaded <a href = \"https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_confirmed_usafacts.csv\">here.</a> For more information on the data, visit <a href = \"https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/\"> this webpage</a> (the data was downloaded on the right side of this page, under \"download data > known cases\". Note that <a href = \"https://usafacts.org\" >usafacts.org</a> itself is not a government website, but rather is a non-profit, non-partisan website that collects government data. Specifically for this project, the data I downloaded is compiled from individual state departments of health. Additionally, this data is displayed on the Centers for Disease Control website, and I confirmed with Bohong that this static file would be acceptable for meeting the \"government data\" requirement. For my first web collection dataset, I used <a href = \"https://api.stlouisfed.org/geofred/series/data?series_id=SCABBE1URN&api_key=19adc513930a610b8f54a46657a55830&date=2020-08-01&file_type=json\">an API from the Federal Reserve Bank of St. Louis </a> which provides unemployment rates in each US county for the month given in the URL. In the code I wrote, I send a total of 9 requests, to scrape data for 9 months (just by altering the month in the URL). You can also view the information the API provides visually (on a map) at <a href = \"https://geofred.stlouisfed.org/map/?th=rdpu&cc=5&rc=false&im=fractile&sb&lng=-90&lat=40&zm=4&sl&sv&am=Average&at=Not%20Seasonally%20Adjusted,%20Monthly,%20Percent,%20no_period_desc&sti=1224&fq=Monthly&rt=county&un=lin&dt=2020-08-01\">this link</a> should you wish to do so. Finally, for my other web collection dataset, I simply scraped <a href = \"https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents\">this Wikipedia page</a> for county populations. <br><br>\n",
    "<b>Q5) For the static file in your dataset collection, state the dimensions of the files in terms of row x columns and the file size (mb, gb, etc.) below. For example, 50,000 rows by 20 columns and 5.4 mb. If your file is a .json file, state the file size (mb, gb, etc.)</b><br>\n",
    "The static file is 3196 rows by 284 columns. It is just under 3 mb, however this data is consistently being updated (new column each day), so it will be 3 mb before the conclusion of Phase II. I discussed the file size with David, who said this file is of an acceptable size for the requirement.<br><br>\n",
    "<b>Q6) Explain why you chose these specific datasets and how they will be used in your analysis. </b><br>\n",
    "I chose these three datasets for two primary reasons: 1) they are essential to analyzing the impact of the pandemic with relevance to economics, and 2) they provide a high level of detail and quantity of data to examine. The daily cumulative case counts by county is necessary because it illustrates the level at which the virus has reached the county and the specific dates on which this occurred. Furthermore, county level analysis provides for a lot more data to analyze, so that we aren't limited to just looking statewide at the 50 states. This allows us to look for deeper regional variations (for instance, so the data from New York City doesn't skew the data for the entire state). I chose to use the Federal Reserve API because unemployment rate is one of the only economic indicators that releases monthly at the county level. In general, economic data is usually either released for higher levels (such as states or for the country collectively) or is released less often. Fortunately, county level unemployment rates are available monthly and only release with about a one month delay, and from an economic standpoint, is considered one of the core indicators (along with GDP and CPI) of economic health, so I used it for this project. As for scraping the populations, while this is a relatively trivial collection and manipulation, this data is necessary in order to generate case rates per population (as the static file only provides raw case numbers), so that counties can be compared regardless of their size. This is essential for being able to complete a proper analysis for case rates in various counties in the country. In general, the idea is to compare case rates for a county with the county's unemployment rate, repeating for each month and each county, to determine what type of relationship exists between these variables. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K0cxaVA1LT-"
   },
   "source": [
    "## About Your Analysis\n",
    "<br>\n",
    "<b>Q7) Provide a list of steps of how you plan on performing your analysis and the way you will gather/present your findings. (Non-technical, high-level overview) </b><br>\n",
    "First, I collected the necessary data, summarizing the cumulative cases per day into cases per month by county, collecting the unemployment rates for each month and mapping them to their counties, and scraping the Wikipedia page for each county's population. Discrepancies are noted, though not removed (more details in the discrepancy section below). Each of these separate collections is exported to their own CSV file, which concludes Phase I of the project. For Phase II, I intend on first mapping all the data to a single CSV file, with each row being a county. Then, I will perform calculations with case rates and then attempt to calculate a correlation between case rates in a given month and unemployment. I also plan on comparing successive months' data, within counties. <br><br>\n",
    "<b>Q8) Explain how you intend on collecting, cleaning, and analyzing the data you gather as well as the manner in which you plan on presenting your insights. (More detailed, include modules, techniques, etc.). </b><br>\n",
    "First, to map all files in the CSV together, I will have to account for different naming conventions between the files. The intent to account for this is to use various combinations of substrings or Regex to try to map all the counties to their counterparts in other files. Once all counties are mapped to one row, I will convert into a DataFrame, such that it is easier to use aggregate methods. This includes calculating case rates (cases as a proportion of population), calculating averages per county and state, and averages across months. For further analysis regarding correlation (which is where I hope to draw a particularly meaningful conclusion), I plan on using Pandas' correlation (DataFrame.corr()) and the graphing capabilities of MatPlotLib. Correlations will be calculated across the variables (that is, case rates to unemployment rates) for each month for each county, and possibly across successive months (to account for the potential, for example, that high Covid rates might take a month or more to make their economic impact). Ultimately, I plan to display scatterplots highlighting a correlation (if one exists) across these variables, and between months. <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSiMishC1LT_"
   },
   "source": [
    "## About You\n",
    "<br>\n",
    "\n",
    "<b>Q9) List the name(s) of those working on this topic/project.</b><br>\n",
    "Nicholas Chen <br><br>\n",
    "<b> Please Initial Below that you acknowledge this statement: <br>\n",
    "I affirm that all of the work in this project will be done my me/my team and is not duplicated from any other source. In addition, any references that I use or code that I choose to model after will be appropriately credited and referenced in my project. </b><br>\n",
    "NC <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fMkCDZe1LUA"
   },
   "source": [
    "## Your Questions (Optional)\n",
    "<br>\n",
    "Please add any clarifying questions that you would like answered below. Follow the same formatting as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjB_SbWY1LUB"
   },
   "source": [
    "## Jupyter Notebook Quick Tips\n",
    "Here are some quick formatting tips to get you started with Jupyter Notebooks. This is by no means exhaustive, and there are plenty of articles to highlight other things that can be done. We recommend using HTML syntax for Markdown but there is also Markdown syntax that is more streamlined and might be preferable. \n",
    "<a href = \"https://towardsdatascience.com/markdown-cells-jupyter-notebook-d3bea8416671\">Here's an article</a> that goes into more detail. (Double-click on cell to see syntax)\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2\n",
    "### Heading 3\n",
    "#### Heading 4\n",
    "<br>\n",
    "<b>BoldText</b> or <i>ItalicText</i>\n",
    "<br> <br>\n",
    "Math Formulas: $x^2 + y^2 = 1$\n",
    "<br> <br>\n",
    "Line Breaks are done using br enclosed in < >.\n",
    "<br><br>\n",
    "Hyperlinks are done with: <a> https://www.google.com </a> or \n",
    "<a href=\"http://www.google.com\">Google</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb9oVjpRDswQ"
   },
   "source": [
    "# Data Collection and Cleaning\n",
    "You are required to provide data collection and cleaning for the three (3) minimum datasets. Create a function for each of the following sections that reads or scrapes data from a file or website, manipulate and cleans the parsed data, and writes the cleaned data into a new file. \n",
    "\n",
    "Make sure your data cleaning and manipulation process is not too simple. Performing complex manipulation and using modules not taught in class shows effort, which will increase the chance of receiving full credit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mRjxZDbE1tj"
   },
   "source": [
    "## Downloaded Dataset Requirement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0p5xxmqzFGrO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "def data_parser():\n",
    "    df = pd.read_csv(\"covid_confirmed_usafacts.csv\")\n",
    "    hasAnyDiscrepancies = []\n",
    "    discrepancyDates = []\n",
    "    for index, row in df.iterrows():\n",
    "        discrepancies = []\n",
    "        hasDiscrepancies = False\n",
    "        prevCol = list(row.iteritems())[4]\n",
    "        for col in list(row.iteritems())[5:]:\n",
    "            if col[1] - prevCol[1] < -10: \n",
    "                # Define a discrepancy to be a day where there are 10 more cumulative cases than the next day\n",
    "                # This allows some tolerance for slight miscounts from day to day. \n",
    "                hasDiscrepancies = True\n",
    "                discrepancies.append(prevCol[0])\n",
    "            prevCol = col\n",
    "        hasAnyDiscrepancies.append(hasDiscrepancies)\n",
    "        discrepancyDates.append(discrepancies)\n",
    "    df.loc[:,\"Big Discrepancy\"] = hasAnyDiscrepancies\n",
    "    df.loc[:,\"Discrepancy Dates\"]=discrepancyDates\n",
    "    #print(list(df.columns))\n",
    "    months = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\", 6:\"June\", 7:\"July\", 8:\"August\",9:\"September\",10:\"October\"}\n",
    "    df[\"Jan New Cases\"]=df.loc[:,\"1/31/20\"]\n",
    "    for i in range (2,11):\n",
    "        lastDayThisMonth = re.findall(str(i)+\"/\\d{2}/20\",\",\".join(df.columns))[-1]\n",
    "        lastDayPrevMonth = re.findall(str(i-1)+\"/\\d{2}/20\",\",\".join(df.columns))[-1]\n",
    "        df[months[i][:3]+\" New Cases\"]=df.loc[:,lastDayThisMonth]-df.loc[:,lastDayPrevMonth]\n",
    "        # Above loops makes the new cases per month column. \n",
    "        # Uses regex to find the last day of this month and the previous month.\n",
    "        # Note the df contains cumulative totals on each day. Therefore, we just need to do last day of this month minus prev\n",
    "    df[\"YTD Total\"]=df.loc[:,lastDayThisMonth]\n",
    "    pd.concat([df.iloc[:,:4], df.loc[:,\"Big Discrepancy\":]], axis=1).to_csv(\"COVIDCasesCountyMonthlySummary.csv\", index=False) \n",
    "    \n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "data_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "794L4vGXFdYw"
   },
   "source": [
    "## Web Collection Requirement \\#1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "# import timeit\n",
    "# from pprint import pprint\n",
    "def web_parser1():\n",
    "    '''\n",
    "    Notes about this function:\n",
    "    The Federal Reserve API releases county level data for previous month's unemployment rate around the end of current month\n",
    "    (that is, September unemployment rate is released throughout end of October into early November)\n",
    "    Therefore, unemployment rates for September may be available for all counties, depending on when this is graded\n",
    "    At the time of this project's completion, only Washington DC had available data for September uneployment, all other counties did not\n",
    "    In general, a handful of counties will \n",
    "    The code is set to handle both cases, and to average Q3 unemployment rates appropriately.\n",
    "    \n",
    "    The code takes some time to run, due to the organization of the output (rows mapping to each county) and the organization\n",
    "    of the Federal Reserve API response (having to request by month, rather than by county). As such, searching through the \n",
    "    data causes the code to be somewhat slow. For me, it took roughly 13 seconds across several trials.\n",
    "    '''\n",
    "    key = \"19adc513930a610b8f54a46657a55830\" # API Key\n",
    "    months = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\", 6:\"June\", 7:\"July\", 8:\"August\",9:\"September\"}\n",
    "    monthlyData = []\n",
    "    for i in range (1,10): # Unemployment rate is only available up to and including August, as of 10/22/20\n",
    "        response = requests.get(f\"https://api.stlouisfed.org/geofred/series/data?series_id=SCABBE1URN&api_key={key}&date=2020-0{str(i)}-01&file_type=json\")\n",
    "        monthlyData.append(response.json()[\"meta\"][\"data\"][\"2020 \"+months[i]])\n",
    "    with open(\"UNRATE2020county.csv\", \"w\") as fout:\n",
    "        writer = csv.writer(fout, lineterminator=\"\\n\")\n",
    "        header = [\"FIPS\",\"County\", \"State\", \"UNRATE-JAN\", \"UNRATE-FEB\", \"UNRATE-MAR\", \"UNRATE-APR\", \"UNRATE-MAY\",\"UNRATE-JUN\",\"UNRATE-JUL\",\"UNRATE-AUG\",\"UNRATE-SEP\", \"UNRATE-Q1\",\"UNRATE-Q2\",\"UNRATE-Q3\",\"Q2-Q1\",\"Q3-Q1\",\"PRESENT-JAN\", \"DECREASE\",\"BIG-DECREASE\"]\n",
    "        writer.writerow(header)\n",
    "        for county in monthlyData[0]: # calls data embeded in like 3 dictionaries\n",
    "            countyList = []\n",
    "            countyList.append(int(county[\"code\"])) # A FIPS code is a 5 digit code representing each county.\n",
    "            countyList.append(county[\"region\"][:-4]) # slices the county name (everything before the  comma for the state)\n",
    "            countyList.append(county[\"region\"][-2:]) # slices the state abbreviation out of each \"region\"\n",
    "            countyList.append(float(county[\"value\"])) # accesses the UNRATE for January\n",
    "            for i in range (2,10): # accesses the UNRATE for the remainder of the months (up to Sept if available, otherwise to Aug)\n",
    "                        added=False # Used to handle missing data for Sept (most regions do not have Sept data available yet)\n",
    "                        for entry in monthlyData[i-1]: \n",
    "                            if(countyList[0]==int(entry[\"code\"])):\n",
    "                                countyList.append(float(entry[\"value\"]))\n",
    "                                added=True\n",
    "                                break # added for efficiency purposes. \n",
    "                        if not added:\n",
    "                            countyList.append(None)\n",
    "            # Below appends summary statistics - average UNRATE per qarter, difference from Q1 for each of Q2 and Q3, and difference Aug to Jan\n",
    "            countyList.append(sum(countyList[3:6])/3)\n",
    "            countyList.append(sum(countyList[6:9])/3)\n",
    "            difference = 0\n",
    "            if None in countyList[9:12]: # Computes Q3 average excluding Sept and computes difference from latest unemp rate to January\n",
    "                countyList.append(sum(countyList[9:11])/2)\n",
    "                difference = countyList[10]-countyList[3]\n",
    "            else:\n",
    "                countyList.append(sum(countyList[9:12])/3)\n",
    "                difference = countyList[11]-countyList[3]\n",
    "            countyList.append(countyList[13]-countyList[12]) # difference between Q2 and Q1, positive indicates higher UNRATE in Q2\n",
    "            countyList.append(countyList[14]-countyList[12]) # difference between Q3 and Q1, positive indicates higher UNRATE in Q3\n",
    "            countyList.append(difference) # difference between latest available month (either Aug or Sept) and Jan 2020 UNRATE\n",
    "            countyList.append(countyList[17]<0)\n",
    "            countyList.append(countyList[17]<-5)\n",
    "            writer.writerow(countyList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "web_parser1()\n",
    "# extime = timeit.timeit(web_parser1, number=1)\n",
    "# print(extime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDD6sMsCXRxc"
   },
   "source": [
    "## Web Collection Requirement \\#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HAkUOqMgXQJG"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "# import timeit\n",
    "# from pprint import pprint\n",
    "def web_parser2():\n",
    "    wiki = requests.get(\"https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents\")\n",
    "    soup = BeautifulSoup(wiki.text, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"class\":\"wikitable sortable\"})\n",
    "    counties = list(table.find_all(\"tr\"))\n",
    "    countyList = []\n",
    "    for county in counties[1:]:\n",
    "        countyInfo = []\n",
    "        name = re.findall('\\\">([A-Za-z (),-–\\.\\']+)</a>',str(county))[0]\n",
    "        if county.find_all(\"span\", {\"class\":\"flagicon\"}) != []: # this indicates that we've reached a new state/territory\n",
    "            state = re.findall('title=\\\"([A-Za-z \\.\\'\\(\\)ʻ]*)\\\"', str(county))[-1] \n",
    "            # Special class of characters created because, apparently, Wikipedia formats some state names with () and ʻ\n",
    "            # index -1 is used to account for special cases in formatting of some terrioties, where find_all actually found multiple matches\n",
    "            state = re.sub(' \\(.+\\)', '', state) \n",
    "            # accounts for cases where (U.S. State) is added at the end of the name, \n",
    "            # which occurs for NY and WA (to differentiate from the cities) and GA (to differentiate from the country)\n",
    "        population = int(re.findall('right;\\\">([0-9,]+)', str(county))[0].replace(\",\",\"\"))\n",
    "        countyInfo.append(name)\n",
    "        countyInfo.append(state)\n",
    "        countyInfo.append(population)\n",
    "        countyList.append(countyInfo)\n",
    "    df = pd.DataFrame(countyList)\n",
    "    us_state_abbrev = {'Alabama': 'AL','Alaska': 'AK','American Samoa': 'AS','Arizona': 'AZ','Arkansas': 'AR','California': 'CA','Colorado': 'CO','Connecticut': 'CT','Delaware': 'DE','District of Columbia': 'DC','Florida': 'FL','Georgia': 'GA','Guam': 'GU','Hawaii': 'HI','Idaho': 'ID','Illinois': 'IL','Indiana': 'IN','Iowa': 'IA','Kansas': 'KS','Kentucky': 'KY','Louisiana': 'LA','Maine': 'ME','Maryland': 'MD','Massachusetts': 'MA','Michigan': 'MI','Minnesota': 'MN','Mississippi': 'MS','Missouri': 'MO','Montana': 'MT','Nebraska': 'NE','Nevada': 'NV','New Hampshire': 'NH','New Jersey': 'NJ','New Mexico': 'NM','New York': 'NY','North Carolina': 'NC','North Dakota': 'ND','Northern Mariana Islands':'MP','Ohio': 'OH','Oklahoma': 'OK','Oregon': 'OR','Pennsylvania': 'PA','Puerto Rico': 'PR','Rhode Island': 'RI','South Carolina': 'SC','South Dakota': 'SD','Tennessee': 'TN','Texas': 'TX','U.S. Minor Outlying Islands' : 'UM','Utah': 'UT','Vermont': 'VT','U.S. Virgin Islands': 'VI','Virginia': 'VA','Washington': 'WA','West Virginia': 'WV','Wisconsin': 'WI','Wyoming': 'WY'}\n",
    "    ''' \n",
    "    us_state_abrev adopted from the dictionary provided by rogerallen on GitHub. \n",
    "    No code logic was taken, only the dictionary mapping states to their abbreviations.\n",
    "    Original available at https://gist.github.com/rogerallen/1583593\n",
    "    I made modifications to include some more territories.\n",
    "    '''\n",
    "    df.columns = [\"County\", \"State\", \"Population\"]\n",
    "    df = df.replace({\"State\":us_state_abbrev}) # replaces state names with abbreviations, to match formatting in other files\n",
    "    df = df.drop(df[(df[\"State\"]==\"VI\")|(df[\"State\"]==\"PR\")|(df[\"State\"]==\"UM\")|(df[\"State\"]==\"MP\")|(df[\"State\"]==\"GU\")|(df[\"State\"]==\"AS\")].index)\n",
    "    # Removes most territories, because unemployment rates and Covid case rates aren't provided for these territories, so no analysis could be performed for these\n",
    "    df.to_csv(\"countyPopulations.csv\", index = False)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "############ Function Call ############\n",
    "web_parser2()\n",
    "# extime = timeit.timeit(web_parser2, number=1)\n",
    "# print(extime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttEYrm9US5s"
   },
   "source": [
    "#Inconsistencies\n",
    "For each inconsistency (NaN, null, duplicate values, empty strings, etc.) you discover in your datasets, write at least 2 sentences stating the significance, how you identified it, and how you handled it.\n",
    "\n",
    "1. The first inconsistencies are logical inconsistencies encountered in the static dataset, which contains cumulative Covid case counts for each county (and statewide unallocated totals) in the United States for each day in the year starting from January 22nd, 2020. Here, a consistency is considered to occur if a county's <i>cumulative</i> case count decreases by more than 10 cases in a single day. Though sometimes, there are slight misallocations of case counts, I consider them to be major discrepancies if the difference is more than a single digit. Such discrepancies are identified by iterating through each day for each county and comparing the current day's cases to the previous day's cases. In these cases, I note that a major discrepancy exists for the county in a column named \"Big Discrepancy\" (containing boolean values of whether or not the county has major case count issues) and note the date(s) in which the issues occur in a column containing a list of dates where such an issue arises. This means that the data is not deleted completely, but merely noted in the event that I want to ignore (or otherwise account for the discrepancy) in Phase II. Most statewide unallocated rows have major (like several hundred case) discrepancies, owing to the nature of unallocated cases (being that they may eventually be allocated to their correct county). Though I won't run analyses with unemployment rates for \"statewide unallocated cases\" (because \"statewide unallocated unemployment rate\" isn't a thing), I did retain the statewide unallocated cases in case I wanted to corroborate significant increases in a county's cases, which could potentially come from allocating previously unallocated cases.\n",
    "\n",
    "\n",
    "2. The next inconsistencies occur in the data from the Federal Reserve API, a particular one being the fact that numerous counties do not yet have unemployment rates released for the month of September. As I mentioned earlier, county level unemployment rates release roughly one month after the month ends, so September unemployment rates are expected to be released within the next few days. Currently (10/28/20), only data for Washington DC is available. Based on the date at which data released for August, I'm expecting most counties to release on 10/30/20 or 10/31/20, with a handful of counties releasing data just before or just after. Either way, this gives rise to the consistency of missing data from the API request for September unemployment rates. To identify these missing rates, I added a boolean in iterating through the API request results that remain False until the county's rate is found for the month. In the CSV file's column for September unemployment rates, I write in a value of None if this boolean remains False, handle the average unemployment for Q3 ignoring this (average of July and August only), and for the difference from present rate to January, use the latest available data (so if September is missing, I use August as present). While this would unfortunately not allow for calculations for the month of September (with their relationship to Covid case rates), I enable at least average calculations to be made for the quarter. <b>NOTE FOR THIS DISCREPANCY:</b> depending on when you grade this component, it is <i>possible</i> that all the data will have been released for September unemployment rates for all counties, thus eliminating this discrepancy. Based on last month's release, this is expected some time between 10/30 and 11/8. As of 10/28/20, data was only available for Washington DC, so this discrepancy did exist when the project was submitted, and all remaining counties were assigned a value of None.\n",
    "\n",
    "\n",
    "3. The other inconsistency in the unemployment rates from the Federal Reserve API is that there are counties whose unemployment rates actually <i>decreased</i> from the beginning of the year until present. These were identified by comparing the difference between current unemployment rates (either August or September, depending on what data is available) to January's unemployment, when we were not yet in a pandemic induced recession. If it was merely a case of a decrease existing (which is possible due to simple fluctuations), I noted it in a boolean column titled \"DECREASE\", and if the unemployment rate in January was more than 5 %-points higher than present, I noted it in a boolean column titled \"BIG DECREASE\". While these are not necessarily logically impossible per se, they represent outliers that could impact the analysis. A quick peek at some of the outliers suggests that these are counties with highly seasonal economies, such as certain counties Alaska (one had a remarkable 19.7% unemployment in January but only 5.9% in August), where most employment is in the summer (for instance, industries such as tourism and fishing). These outliers could impact the analysis or they could average out with counties with seasonal economies in the opposite direction, so while I did not remove them, I did note their existence, so that I could run the analysis with and/or without them during Phase II.\n",
    "\n",
    "\n",
    "4. (if applicable)\n",
    "\n",
    "\n",
    "5. (if applicable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp7Pm-Suh3d"
   },
   "source": [
    "## Data Sources\n",
    "Include sources (as links) to your datasets.\n",
    "*   Downloaded Dataset Source: <a>https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_confirmed_usafacts.csv</a>\n",
    "*   Web Collection #1 Source: <a>https://api.stlouisfed.org/geofred/series/data?series_id=SCABBE1URN&api_key=19adc513930a610b8f54a46657a55830&date=2020-08-01&file_type=json</a> Note that this is the specific API request for the month of August. In the code, I send a request for each month from January to September. You can access a specific month by changing the month in the URL. Also, this data is available visually (on a map), which you can view at <a>https://geofred.stlouisfed.org/map/?th=rdpu&cc=5&rc=false&im=fractile&sb&lng=-90&lat=40&zm=4&sl&sv&am=Average&at=Not%20Seasonally%20Adjusted,%20Monthly,%20Percent,%20no_period_desc&sti=1224&fq=Monthly&rt=county&un=lin&dt=2020-08-01</a>\n",
    "*   Web Collection #2 Source: <a>https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents</a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhaseI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
